{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lstm = pd.read_csv('../predictions/y_pred_lstm.csv')\n",
    "y_pred_arima = pd.read_csv('../predictions/y_pred_arima.csv')\n",
    "y_pred_lightgbm = pd.read_csv('../predictions/y_pred_lightgbm.csv')\n",
    "y_pred_prophet = pd.read_csv('../predictions/y_pred_prophet.csv')\n",
    "y_pred_sarima = pd.read_csv('../predictions/y_pred_sarima.csv')\n",
    "y_pred_extra_trees = pd.read_csv('../predictions/y_pred_extra_trees.csv')\n",
    "y_pred_xgboost = pd.read_csv('../predictions/y_pred_xgboost.csv')\n",
    "y_pred_random_forest = pd.read_csv('../predictions/y_pred_random_forest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the true labels for the test set\n",
    "y_test = pd.read_csv('../predictions/y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, rmse, r2\n",
    "\n",
    "metrics_lstm = evaluate_model(y_test, y_pred_lstm)\n",
    "metrics_arima = evaluate_model(y_test, y_pred_arima)\n",
    "metrics_lightgbm = evaluate_model(y_test, y_pred_lightgbm)\n",
    "metrics_prophet = evaluate_model(y_test, y_pred_prophet)\n",
    "metrics_sarima = evaluate_model(y_test, y_pred_sarima)\n",
    "metrics_extra_trees = evaluate_model(y_test, y_pred_extra_trees)\n",
    "metrics_xgboost = evaluate_model(y_test, y_pred_xgboost)\n",
    "metrics_random_forest = evaluate_model(y_test, y_pred_random_forest)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['LSTM', 'ARIMA', 'LightGBM Regressor', 'Prophet', 'S-ARIMA', 'Extra Tree Regressor', 'XGBoost', 'Random Forest Regressor'],\n",
    "    'MAE': [metrics_lstm[0], metrics_arima[0], metrics_lightgbm[0], metrics_prophet[0], metrics_sarima[0], metrics_extra_trees[0], metrics_xgboost[0], metrics_random_forest[0]],\n",
    "    'RMSE': [metrics_lstm[1], metrics_arima[1], metrics_lightgbm[1], metrics_prophet[1], metrics_sarima[1], metrics_extra_trees[1], metrics_xgboost[1], metrics_random_forest[1]],\n",
    "    'R2': [metrics_lstm[2], metrics_arima[2], metrics_lightgbm[2], metrics_prophet[2], metrics_sarima[2], metrics_extra_trees[2], metrics_xgboost[2], metrics_random_forest[2]]\n",
    "})\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.plot(y_test, label='Actual', color='black')\n",
    "plt.plot(y_pred_lstm, label='LSTM', alpha=0.7)\n",
    "plt.plot(y_pred_arima, label='ARIMA', alpha=0.7)\n",
    "plt.plot(y_pred_lightgbm, label='LightGBM Regressor', alpha=0.7)\n",
    "plt.plot(y_pred_prophet, label='Prophet', alpha=0.7)\n",
    "plt.plot(y_pred_sarima, label='S-ARIMA', alpha=0.7)\n",
    "plt.plot(y_pred_extra_trees, label='Extra Tree Regressor', alpha=0.7)\n",
    "plt.plot(y_pred_xgboost, label='XGBoost', alpha=0.7)\n",
    "plt.plot(y_pred_random_forest, label='Random Forest Regressor', alpha=0.7)\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Actual vs Predicted Energy Consumption')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.plot(y_test[:100].values, label='Target value',color='b')\n",
    "plt.plot(y_pred_extra_trees[:100], label='Extra Tree', linestyle='--', color='y')\n",
    "plt.plot(y_pred_lightgbm[:100], label='LightGBM', linestyle='--', color='g')\n",
    "plt.plot(y_pred_arima[:100], label='ARIMA', linestyle='--', color='r')\n",
    "plt.plot(y_pred_lstm[:100], label='LSTM', linestyle='--', color='black')\n",
    "plt.plot(y_pred_prophet[:100], label='Prophet', linestyle='--', color='orange')\n",
    "plt.plot(y_pred_random_forest[:100], label='Random Forest', linestyle='--', color='purple')\n",
    "plt.plot(y_pred_sarima[:100], label='S-ARIMA', linestyle='--', color='gray')\n",
    "plt.plot(y_pred_xgboost[:100], label='XGBoost', linestyle='--', color='pink')\n",
    "\n",
    "plt.legend(loc=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
